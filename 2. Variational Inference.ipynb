{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p float=\"left\">\n",
    "  <img style=\"float: right;\" src=\"./imgs/logo1.png\" width=\"100\" />\n",
    "  <img style=\"float: left;\" src=\"./imgs/ipa.png\" width=\"300\" /> \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/div.png\" width=\"100%\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "# 2. Variational Inference\n",
    "2.1 Einführung: Modellierung eines <span style=\"color:#179c80\"><b>Münzwurfs</b></span> <br>\n",
    "2.2 Bestimmung der <span style=\"color:#179c80\"><b>Abweichung</b></span> zwischen Verteilungen <br>\n",
    "2.3 Schätzen von Verteilungen über <span style=\"color:#179c80\"><b>Optimierungsverfahren</b></span><br>\n",
    "2.4 <span style=\"color:#179c80\"><b>Complexity Cost</b></span> im Beispiel Münzwurf <br>\n",
    "2.5 <span style=\"color:#179c80\"><b>Likelihood Cost</b></span> im Beispiel Münzwurf <br>\n",
    "2.7 <span style=\"color:#179c80\"><b>Numerisches</b></span> Verfahren zur Lösung des Problems\n",
    "\n",
    "\n",
    "\n",
    "## 2.1 Einführung: Modellierung eines Münzwurfs\n",
    "Wie auch im vorherigen MCMC Experiment, schauen wir uns das Beispiel eines Münzwurfs an.\n",
    "Unser <span style=\"color:#179c80\"><b>Ziel</b></span> ist also wieder die Berechnung der Wahrscheinlichkeit des Events <span style=\"color:#db8642\">Kopf</span> eines Münzwurfs, unter Berücksichtigung von Daten. Formell sind wir  an $p(\\text{Kopf}|\\text{Daten})$ bzw. $p(\\theta|y)$ interessiert. Wieder konnten wir bei einem <span style=\"color:#db8642\">zehn</span>-fachen Münzwurfs  <span style=\"color:#db8642\">vier</span> mal Kopf beobachten. Die Parameter unseres Beta-<span style=\"color:#179c80\"><b>Priors</b></span> definieren wir  mit $\\alpha_{\\text{pr}}=1, \\beta_{\\text{pr}}=1$, also\n",
    "$$p(\\theta) = \\text{Beta}(\\alpha_{\\text{pr}}, \\beta_{\\text{pr}}).$$\n",
    "\n",
    "\n",
    "Aus dem vorherigen Tutorial wissen wir, dass sich eine $Beta(\\alpha_{\\text{po}}, \\beta_{\\text{po}})$ Verteilung mit $\\alpha_{\\text{po}}=5, \\beta_{\\text{po}}=7$  <span style=\"color:#db8642\">analytisch</span> als <span style=\"color:#179c80\"><b>Posterior</b></span> berechnen lässt. Typischerweise ist jedoch die Posterior Verteilung nur sehr <span style=\"color:#db8642\">schwer</span> exakt berechenbar. Hier setzt Variational Inference an und definiert eine <span style=\"color:#db8642\">Ersatz</span>-Verteilung für die Posterior, welche einfacher zu berechnen ist - die <span style=\"color:#179c80\"><b>Variational</b></span> Verteilung $q(\\theta)$. Über ein <span style=\"color:#179c80\"><b>Optimierungsverfahren</b></span> werden dann die <span style=\"color:#179c80\"><b>optimalen</b></span> Parameter von $q(\\theta)$ gesucht, um den Unterschied zwischen $q(\\theta)$ und der wahren Posterior, $p(\\theta|y)$, zu minimieren. Der Einfachheit halber wählen wir \n",
    "\n",
    "$$q(\\theta) = \\text{Beta}(\\alpha_{\\text{va}}, \\beta_{\\text{va}}).$$\n",
    "\n",
    "Für $\\alpha_{\\text{va}}=5$ und  $\\beta_{\\text{va}}=7$ bei einem Prior von Beta(1,1)  wissen wir dann, dass die Abweichung zwischen $q(\\theta)$ und $p(\\theta|y)$ <span style=\"color:#db8642\">minimal</span> ist - nämlich 0. Es wird also die Aufgabe sein, über Variational Inference genau diese <span style=\"color:#db8642\">Parameter</span> zu berechnen.  Doch wie misst man eigentlich die  <span style=\"color:#179c80\"><b>Abweichung</b></span> zweier Verteilungen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/div.png\" width=\"100%\" /> \n",
    "\n",
    "## 2.2 Bestimmung der Abweichung zwischen Verteilungen\n",
    "\n",
    "Eine Funktion, die <span style=\"color:#db8642\">Wahrscheinlichkeitsverteilungen</span> als Eingabe nimmt und eine Zahl zurückgibt, die angibt, wie sehr sie sich die Verteilungen <span style=\"color:#db8642\">unterscheiden</span> wird in der Statistik als <span style=\"color:#179c80\"><b>Divergenz</b></span> bezeichnet.  Die zurückgegebene Zahl darf <span style=\"color:#db8642\">nicht</span> negativ sein und ist <span style=\"color:#db8642\">nur</span> dann gleich Null, wenn die beiden Verteilungen identisch sind. <span style=\"color:#db8642\">Größere</span> Zahlen zeigen eine größere Unähnlichkeit an. Eine der bekanntesten Divergenzen ist die <span style=\"color:#db8642\">Kullback-Leibler</span> Divergenz $D_{KL}$, die typischerweiße zum Einsatz kommt wenn die <span style=\"color:#db8642\">Unterschiedlichkeit</span> zwischen einer vorhandenen präzisen Wahrscheinlichkeitsverteilung $p$ und einer Approximierung $q$ bestimmt werden soll. Es gilt,\n",
    "\n",
    "$$D_{KL}(q||p) = \\int q(\\theta) \\cdot \\text{log} \\frac{q(\\theta)}{p(\\theta)} d\\theta$$\n",
    "\n",
    "Ob eine analytisch exakte Lösung dieses Integrals existiert hängt stark von den <span style=\"color:#db8642\">Typen</span> der jeweiligen Verteilungen ab. Damit Sie ein besseres Gefühl für die KL-Divergenz erhalten, können Sie gerne mit folgendem Beispiel zweier <span style=\"color:#db8642\">Gauß-Verteilungen</span> experimentieren. Die Berechnung für die KL-Divergenz zwischen zwei univariaten Gauß-Verteilungen wird in diesem <a href=\"https://stats.stackexchange.com/questions/7440/kl-divergence-between-two-univariate-gaussians\">Post</a> sehr schön beschrieben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Starten der GUI [Keine Änderung notwendig]\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "import ipywidgets\n",
    "from ipywidgets import HBox, VBox, RadioButtons, Layout\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sbn\n",
    "from scipy.stats import beta, binom, norm\n",
    "import numpy as np\n",
    "from ipywidgets import *\n",
    "from helper.debounce import debounce\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import set_matplotlib_formats\n",
    "%matplotlib inline\n",
    "import math\n",
    "import jdc\n",
    "import scipy.special as scf\n",
    "import scipy.stats as scs\n",
    "from scipy.stats import beta, binom, norm\n",
    "import scipy.optimize as sco\n",
    "import seaborn as sbn\n",
    "sbn.set()\n",
    "# imports\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "import ipywidgets\n",
    "from ipywidgets import HBox, VBox, RadioButtons, Layout\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sbn\n",
    "from scipy.stats import beta, binom, norm\n",
    "import numpy as np\n",
    "from ipywidgets import *\n",
    "from helper.debounce import debounce\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import set_matplotlib_formats\n",
    "%matplotlib inline\n",
    "import math\n",
    "import jdc\n",
    "import scipy.special as scf\n",
    "import scipy.stats as scs\n",
    "from scipy.stats import beta, binom, norm\n",
    "import scipy.optimize as sco\n",
    "# Code\n",
    "sl_m1 = widgets.FloatSlider(value=-2, min=-3, max=3, step=0.1, description=\"$\\mu_1$\")\n",
    "sl_v1 = widgets.FloatSlider(value=0.3, min=0.2, max=0.6, step=0.05, description=\"$\\sigma_1$\")\n",
    "sl_m2 = widgets.FloatSlider(value=2, min=-3, max=3, step=0.1, description=\"$\\mu_2$\")\n",
    "sl_v2 = widgets.FloatSlider(value=0.5, min=0.2, max=0.6, step=0.05, description=\"$\\sigma_2$\")\n",
    "out = widgets.Output()\n",
    "\n",
    "@debounce(0.2)\n",
    "def draw(obj):\n",
    "    def kld_gauss(u1, s1, u2, s2):\n",
    "          # general KL two Gaussians\n",
    "          # u2, s2 often N(0,1)\n",
    "          # https://stats.stackexchange.com/questions/7440/ +\n",
    "          # kl-divergence-between-two-univariate-gaussians\n",
    "          # log(s2/s1) + [( s1^2 + (u1-u2)^2 ) / 2*s2^2] - 0.5\n",
    "          v1 = s1 * s1\n",
    "          v2 = s2 * s2\n",
    "          a = np.log(s2/s1) \n",
    "          num = v1 + (u1 - u2)**2\n",
    "          den = 2 * v2\n",
    "          b = num / den\n",
    "          return a + b - 0.5\n",
    "    x = np.linspace(-3, 3, 500)\n",
    "    p = norm.pdf(x, sl_m1.value, sl_v1.value)\n",
    "    q = norm.pdf(x, sl_m2.value, sl_v2.value)\n",
    "    \n",
    "    p[p<=0.001] = np.nan\n",
    "    q[q<=0.001] = np.nan\n",
    "    \n",
    "    \n",
    "    \n",
    "    kl = kld_gauss(sl_m1.value, sl_v1.value, sl_m2.value, sl_v2.value)\n",
    "    with out:\n",
    "        fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(10, 3))\n",
    "        ax.plot(x, p, linewidth=3, color=\"#179c80\")\n",
    "        ax.plot(x, q, linewidth=3, color=\"#db8642\")\n",
    "        ax.set_xlim([-3.1, 3.1])\n",
    "        ax.set_ylim([-0.1, 3.1])\n",
    "        ax.set_title(f\"Kullback-Leibler Divergenz: {'%.2f'%kl}\", fontsize=15)\n",
    "        clear_output(wait=True)\n",
    "        plt.show(fig)\n",
    "    \n",
    "draw(None)\n",
    "for obj in [sl_m1, sl_v1, sl_m2, sl_v2]:\n",
    "    obj.observe(draw, names=\"value\")\n",
    "widgets.HBox([widgets.VBox([widgets.Label(\"Gauß-Verteilung Momente\"), widgets.VBox([sl_m1, sl_v1]), widgets.Label(\"\"), widgets.VBox([sl_m2, sl_v2])]), out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/div.png\" width=\"100%\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "## 2.3 Schätzen von Verteilungen über Optimierungsverfahren\n",
    "\n",
    "Nun können wir die Idee der <span style=\"color:#179c80\"><b>Divergenz</b></span> auf unseren Münzwurf übertragen. Das Ziel ist folgende Gleichung <span style=\"color:#179c80\"><b>Iterativ</b></span> zu lösen:\n",
    "\n",
    "$$\\underset{\\alpha_{\\text{va}}, \\beta_{\\text{va}}}{\\text{argmin}} \\  D_{KL}(\\underbrace{q(\\theta)}_{\\text{variational}}||\\underbrace{p(\\theta|y)}_{\\text{posterior}}).$$\n",
    "\n",
    "Eine genaue Herleitung des Divergenz Terms übersteigt den Rahmen dieses Notebooks - für uns genügt es zu wissen, dass die Integralschreibweise der <span style=\"color:#179c80\"><b>KL-Divergenz</b></span> auch mithilfe von <span style=\"color:#db8642\">drei</span> Termen dargestellt werden kann:\n",
    "\n",
    "$$D_{KL}(q(\\theta)||p(\\theta|y)) = \\underbrace{KL[q(\\theta)||p(\\theta)]}_{\\text{1. Kosten-Term}} \\ - \\  \\underbrace{\\mathbb{E}_q[\\text{log}\\ p(\\text{y}|\\theta)]}_{\\text{2. Kosten-Term}} \\ - \\  \\text{log} \\underbrace{p(y)}_{\\text{Evidenz}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <div class=\"alert alert-block alert-info\" style=\"border-color:white\">\n",
    "    <b>Aufgabe</b>: Betrachten Sie die drei Terme aus denen sich die <em>KL-Divergenz</em> zwischen <em>Variational</em> und <em>Posterior</em> Verteilung zusammensetzt und beantworten Sie folgende Fragen:\n",
    "    <br>\n",
    "    <b>a)</b> Warum ist Variational Inference berechenbar, obwohl doch der dritte Term der (meistens) sehr hoch-dimensionalen Evidenz entspricht, welcher üblicherweise eine exakte Lösung verhindert?\n",
    "    <br>\n",
    "    <b>b)</b> Die <em>KL-Divergenz</em> setzt sich aus aus einem <em>Likelihood Cost</em> und <em>Complexity Cost</em> zusammen - weisen Sie diese Begriffe den entsprechenden Termen der Gleichung zu und erklären Sie was durch eine Minimierung des gesamten Terms intuitiv erreicht wird.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Antwort</b>: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### <div class=\"alert alert-block alert-success\" style=\"background-color:#b6dad2;color:#179c80;border-color:white\"> <b>Lösung</b> <small style=\"color:#179c80\">(Klicken auf den Pfeil um die Lösung aufzuklappen)</small></span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "> <b>a)</b> Variational Inference sucht nach den Parametern der Variational Verteilung, sodass die Unterschiedlichkeit zwischen der wahren Posterior Verteilung und der definierten Variational Verteilung möglichst gering wird. Da der Evidenz Term <span style=\"color:#db8642\">konstant</span> ist, spielt dieser im Optimierunsverfahren keine Rolle. Mit anderen Worten: die Operation <em>argmin</em> liefert die <span style=\"color:#db8642\">gleichen</span>  Werte für die Parameter unserer Variational Verteilung, unabhängig davon ob man die <span style=\"color:#db8642\">unnormalisierte</span> Posterior Verteilung (ohne Einbeziehung der Evidenz) verwendet, oder die <span style=\"color:#db8642\">wahre</span>.\n",
    "<br>\n",
    "\n",
    "> <b>b)</b> Der 1. Kosten-Term wird als <span style=\"color:#db8642\">Complexity Cost</span> bezeichnet und misst die Abweichung zwischen der aktuellen Variational Verteilung und und dem Prior. Der 2. Kosten-Term wird als  <span style=\"color:#db8642\">Likelihood Cost</span> bezeichnet und entspricht dem negativen Erwartungswert der Log-Likelihood der Daten. Um obige Gleichung zu minimieren gibt es als zwei Stellschrauben:\n",
    "    <ul>\n",
    "    <li><span style=\"color:#db8642\">Minimierung</span> der KL-Divergenz zwischen der Variational Verteilung und dem Prior. Intuitiv bedeuted dies, dass man die Komplexität des Modells gering halten möchte und möglichst nahe bei der Prior Verteilung bleibt.</li>\n",
    "    <li><span style=\"color:#db8642\">Maximierung</span> des negativen Erwartungswert der Log-Likelihood. Intuitiv bedeuted dies, dass man ein Modell sucht, welches die vorliegenden Daten gut beschreibt.</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "<img src=\"./imgs/div.png\" width=\"100%\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "## 2.4 Complexity Cost im Beispiel Münzwurf \n",
    "\n",
    "Um unsere Problemstellung des Münzwurfs mithilfe Variational Inference zu lösen, bedarf es also zum einen der Berechnung der <span style=\"color:#179c80\"><b>KL-Divergenz</b></span> zwischen unserer <span style=\"color:#db8642\">Variational</span> Verteilung und unserem <span style=\"color:#db8642\">Prior</span>. Auch hier gilt: die Lösung ist <span style=\"color:#db8642\">nicht immer</span> analytisch gegeben. Jedoch unter der <span style=\"color:#db8642\">Annahme</span>, dass $q(\\theta)$ und $p(\\theta)$ beide <span style=\"color:#179c80\"><b>Beta</b></span>-verteilt sind, lässt sich eine <span style=\"color:#db8642\">geschlossene</span> Form des Terms ableiten. \n",
    "\n",
    "<br> \n",
    "Wenn  $q(\\theta)$ durch $\\alpha_{\\text{va}}$ und $\\beta_{\\text{va}}$ und $p(\\theta)$ durch $\\alpha_{\\text{pr}}$ und $\\beta_{\\text{pr}}$ parametrisiert werden ergibt sich:\n",
    "\n",
    "$$D_{KL}[q(\\theta)||p(\\theta)] = \\underbrace{\\text{log} \\frac{B(\\alpha_{\\text{pr}}, \\beta_{\\text{pr}})}{B(\\alpha_{\\text{va}}, \\beta_{\\text{va}})}}_{\\text{a}} + \\underbrace{(\\alpha_{\\text{va}} - \\alpha_{\\text{pr}})\\cdot\\psi(\\alpha_{\\text{va}})}_{\\text{b}} + \\underbrace{(\\beta_{\\text{va}} - \\beta_{\\text{pr}})\\cdot\\psi(\\beta_{\\text{va}})}_{\\text{c}} + \\underbrace{(\\alpha_{\\text{pr}} - \\alpha_{\\text{va}} + \\beta_{\\text{pr}}-\\alpha_{\\text{va}})\\cdot\\psi(\\alpha_{\\text{pr}}+\\beta_{\\text{pr}})}_{\\text{d}},$$\n",
    "\n",
    "wobei $B$ die <span style=\"color:#179c80\"><b>Beta</b></span>-Funktion und $\\psi$ die <span style=\"color:#179c80\"><b>Digamma</b></span>-Funktion ist.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"  style=\"border-color:white\">\n",
    "    <b>Aufgabe</b>: Setzen Sie die Formel der <em>KL-Divergenz</em> in der Methode <em>KL_beta</em> um. Teilen Sie die Berechnung in die Terme <em>a</em>, <em>b</em> , <em>c</em>  und <em>d</em>  auf. Anschließend können Sie ihre Umsetzung mit den darauffolgenden Zellen testen.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\" style=\"background-color:#f2decc\n",
    ";border-color:white\">\n",
    "    <b>Hilfreiche Funktionen</b>: \n",
    "    <ul>\n",
    "        <li> <em>scf.beta(alpha, beta)</em>: Beta-Funktion </li>\n",
    "    <li> <em>scf.psi(x)</em>: Digamma Funktion </li>\n",
    "        <li> <em>np.log()</em> </li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\" style=\"border-color:white\">\n",
    "    <b>Wichtig!</b>: Bei Änderungen der <em>KL_beta</em> Methode, führen sie die <b>vier</b> nachfolgenden Zellen erneut aus.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Erstellung der VI Klasse und GUI Elemente [Keine Änderung notwendig]\n",
    "class VI():\n",
    "    def __init__(self, alpha_pr=1, beta_pr=1, heads=4, n=10):\n",
    "        self.alpha_pr = alpha_pr\n",
    "        self.beta_pr = beta_pr\n",
    "        self.heads = heads\n",
    "        self.n = n\n",
    "        self.res, self.cost = None, []\n",
    "        self.q_trace = []\n",
    "        self.counter = 0\n",
    "        self.param_trace = []\n",
    "        self.kl , self.E_log = 0, 0\n",
    "        with out:\n",
    "            sbn.set()\n",
    "            self.fig, self.ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 5))\n",
    "            plt.show(self.fig)\n",
    "    def plot(self, btn):\n",
    "        with out:\n",
    "            if self.counter >= len(self.param_trace):\n",
    "                btn.disabled=True\n",
    "            else:\n",
    "                sbn.set()\n",
    "                label_alpha.value=r\"{:.3f}\".format(self.param_trace[self.counter][0])\n",
    "                label_beta.value=r\"{:.3f}\".format(self.param_trace[self.counter][1])\n",
    "                label_kl.value=r\"{:.3f}\".format(self.cost[self.counter][0])\n",
    "                label_log_e.value = r\"{:.3f}\".format(self.cost[self.counter][1])\n",
    "                kl_va_po = self.KL_beta(self.param_trace[self.counter][0], self.param_trace[self.counter][1], \n",
    "                                   5, 7)\n",
    "                label_kl2.value=r\"{:.3f}\".format(kl_va_po)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                self.fig, self.ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 5))\n",
    "                # plot posterior\n",
    "                xx = np.linspace(0, 1, 1000)\n",
    "                self.ax.plot(xx, beta.pdf(xx, 5, 7), color=\"#179c80\", label=\"Posterior\", linewidth=3)\n",
    "                self.ax.plot(xx, beta.pdf(xx, self.alpha_pr, self.beta_pr), label=\"Prior\", color=\"#db8642\", linewidth=3)\n",
    "                self.ax.plot(np.linspace(0, 1, 250), self.q_trace[self.counter], linestyle=\"--\", label=\"Variational\", linewidth=3)\n",
    "                self.counter += 1\n",
    "                self.ax.legend()\n",
    "                self.ax.set_ylim([-0.1, 3.1])\n",
    "                clear_output(wait=True)\n",
    "                plt.show(self.fig)\n",
    "            \n",
    "    \n",
    "    def reset(self):\n",
    "        progr.value  = 0\n",
    "        self.res, self.cost = None, []\n",
    "        self.q_trace = []\n",
    "        self.counter = 0\n",
    "        self.param_trace = []\n",
    "        self.kl , self.E_log = 0, 0\n",
    "        with out:\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to VI\n",
    "def KL_beta(self, alpha_va, beta_va, alpha_pr, beta_pr):\n",
    "    #a = ...\n",
    "    #b = ...\n",
    "    #c = ...\n",
    "    #d = ...\n",
    "    \n",
    "    return -1 # geben sie den richtigen Wert zurück"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Test: muss 0 ergeben\n",
    "print('%.3f' % VI(alpha_pr=1, beta_pr=1, heads=4, n=10).KL_beta(1, 1, 1, 1)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Test: muss 1.791 ergeben\n",
    "print('%.3f' % VI(alpha_pr=1, beta_pr=1, heads=4, n=10).KL_beta(0.5, 0.2, 1, 1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### <div class=\"alert alert-block alert-success\" style=\"background-color:#b6dad2;color:#179c80;border-color:white\"> <b>Lösung</b> <small style=\"color:#179c80\">(Klicken auf den Pfeil um die Lösung aufzuklappen)</small></span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "%%add_to VI\n",
    "def KL_beta(self, alpha_va, beta_va, alpha_pr, beta_pr):\n",
    "    a = np.log(scf.beta(alpha_pr, beta_pr) / scf.beta(alpha_va, beta_va))\n",
    "    b = (alpha_va - alpha_pr) * scf.psi(alpha_va)\n",
    "    c = (beta_va - beta_pr) * scf.psi(beta_va)\n",
    "    d = (alpha_pr - alpha_va + beta_pr - beta_va) * scf.psi(alpha_va + beta_va)\n",
    "    \n",
    "    return a+b+c+d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "<img src=\"./imgs/div.png\" width=\"100%\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "## 2.5 Likelihood Cost im Beispiel Münzwurf \n",
    "\n",
    "Es sollte nicht überraschend sein, dass es nur selten eine exakte Lösung für $\\mathbb{E}_q[\\text{log}\\ p(\\text{daten}|\\theta)]$ gibt. Geschickt, dass wir unter der Verwendung von <span style=\"color:#179c80\"><b>Beta</b></span>-Funktionen, wie auch für die KL-Divergenz, eine <span style=\"color:#db8642\">geschlossene</span> Form herleiten können. Diese sieht wie folgt aus:\n",
    "\n",
    "$$\\mathbb{E}_q[\\text{log}\\ p(y|\\theta)] = \\underbrace{\\text{Anzahl Kopf}\\cdot(\\psi(\\alpha_{\\text{var}})-\\psi(\\alpha_{\\text{var}}+\\beta_{\\text{var}}))}_{\\text{a}} + \\underbrace{\\text{Anzahl Zahl}\\cdot(\\psi(\\beta_{\\text{var}})-\\psi(\\alpha_{\\text{var}}+\\beta_{\\text{var}}))}_{\\text{b}},$$\n",
    "\n",
    "wobei sich $\\text{Anzahl Kopf}$ und $\\text{Anzahl Zahl}$ jeweils auf die <span style=\"color:#db8642\">Anzahl</span> der entsprechenden Ereignisse des <span style=\"color:#db8642\">zehn</span>-maligen Münzwurfs beziehen - also <span style=\"color:#179c80\"><b>vier</b></span> und <span style=\"color:#179c80\"><b>sechs</b></span>. \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"  style=\"border-color:white\">\n",
    "    <b>Aufgabe</b>: Setzen Sie die Formel in der Methode <em>E_log_likelihood</em> um. Teilen Sie die Berechnung in die Terme <em>a</em> und <em>b</em>  auf. Anschließend können Sie ihre Umsetzung mit den darauffolgenden Zellen testen.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\" style=\"background-color:#f2decc\n",
    ";border-color:white\">\n",
    "    <b>Wichtige Klassenattribute</b>: \n",
    "    <ul>\n",
    "        <li> <em>self.heads</em>: Anzahl des Ereignisses Kopf (4) </li>\n",
    "    <li> <em>self.n</em>: Anzahl der Wiederholung des Münzwurfs (10) </li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\" style=\"border-color:white\">\n",
    "    <b>Wichtig!</b>: Bei Änderungen der <em>KL_beta</em> Methode, führen sie die <b>vier</b> nachfolgenden Zellen erneut aus.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to VI\n",
    "def E_log_likelihood(self, alpha_va, beta_va):\n",
    "    #a = ...\n",
    "    #b = ...\n",
    "    return -1 # geben sie den richtigen Wert zurück"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Test: muss -10.000 ergeben\n",
    "print('%.3f' % VI(alpha_pr=1, beta_pr=1, heads=4, n=10).E_log_likelihood(1, 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Test: muss -8.833 ergeben\n",
    "print('%.3f' % VI(alpha_pr=1, beta_pr=1, heads=4, n=10).E_log_likelihood(3, 2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### <div class=\"alert alert-block alert-success\" style=\"background-color:#b6dad2;color:#179c80;border-color:white\"> <b>Lösung</b> <small style=\"color:#179c80\">(Klicken auf den Pfeil um die Lösung aufzuklappen)</small></span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "%%add_to VI\n",
    "def E_log_likelihood(self, alpha_va, beta_va):\n",
    "    a = self.heads * (scf.psi(alpha_va) - scf.psi(alpha_va + beta_va))\n",
    "    b = (self.n-self.heads) * (scf.psi(beta_va) - scf.psi(alpha_va + beta_va))\n",
    "    return a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "<img src=\"./imgs/div.png\" width=\"100%\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "## 2.6 Numerische Verfahren zur Lösung des Problems\n",
    "\n",
    "Aus den nun implementierten Methoden können wir uns die Funktion bauen, welche es in der <span style=\"color:#179c80\"><b>Optimierung</b></span> zu minimieren gilt. Diesen Part haben wir bereits übernommen - gerne können Sie sich aber die nachfolgende  <span style=\"color:#db8642\">Methode</span> etwas genauer anschauen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "%%add_to VI\n",
    "def objective(self, param_va):\n",
    "    self.kl = self.KL_beta(param_va[0], param_va[1], self.alpha_pr, self.beta_pr)\n",
    "    self.E_log = self.E_log_likelihood(param_va[0], param_va[1])\n",
    "    progr.value += 1\n",
    "    return self.kl - self.E_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun gibt es viele verschiedene Möglichkeiten eine Funktion iterativ zu <span style=\"color:#179c80\"><b>minimieren</b></span>. Typischerweise basiert  <span style=\"color:#db8642\">Variational Inference</span> bei nur schwer lösbarer KL-Divergenz bzw. nur schwer lösbaren Erwartungswerten auf <span style=\"color:#179c80\"><b>Gradienten</b></span>-basierte Verfahren. Ebenso werden bei sehr großen Datensätzen diese Gradienten nur auf einem Teil des Datensatz berechnet (Stochastic Variational Inference). Für unser einfaches Beispiel bedienen wir uns dem <em>scipy.optimize.minimize</em> package und übergeben als Funktion die oben stehende Methode <em>objective</em>. Über die  <span style=\"color:#db8642\">grafisch</span> Benutzeroberfläche können Sie  beobachten, wie  <span style=\"color:#db8642\">Schritt für Schritt</span>  die <span style=\"color:#179c80\"><b>Divergenz</b></span> zwischen Variational und Posterior Verteilung minimiert wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "%%add_to VI\n",
    "def run(self, btn):\n",
    "    info_label.value = \"\"\n",
    "    def cl(param_va):\n",
    "        self.param_trace.append(param_va)\n",
    "        self.cost.append([self.kl, self.E_log])\n",
    "        self.q_trace.append(scs.beta(param_va[0], param_va[1]).pdf(z))\n",
    "    \n",
    "    self.reset()\n",
    "    \n",
    "    z = np.linspace(0, 1, 250)\n",
    "    self.q_trace.append(scs.beta(self.alpha_pr, self.beta_pr).pdf(z))\n",
    "    self.param_trace.append([self.alpha_pr, self.beta_pr])\n",
    "    self.cost.append([0, self.E_log_likelihood(self.alpha_pr, self.beta_pr)])\n",
    "    self.res = sco.minimize(self.objective, [self.alpha_pr, self.beta_pr],\n",
    "                   callback=cl)\n",
    "    info_label.value = self.res.message\n",
    "    progr.value  = progr.max\n",
    "    btn2.disabled = False\n",
    "    self.plot(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Starten der GUI [Keine Änderung notwendig]\n",
    "btn = widgets.Button(description=\"Start Optimierung\")\n",
    "btn2 = widgets.Button(description=\"Nächter Schritt\")\n",
    "progr = widgets.IntProgress()\n",
    "out = widgets.Output()\n",
    "vi = VI()\n",
    "btn.on_click(vi.run)\n",
    "btn2.on_click(vi.plot)\n",
    "label_alpha_1 = widgets.Label(value=rf\"$\\alpha_{{\\text{{var}}}}=$\")\n",
    "label_alpha = widgets.Label(value=rf\"0.000\")\n",
    "\n",
    "label_beta_1 = widgets.Label(value=rf\"$\\beta_{{\\text{{var}}}}=$\")\n",
    "label_beta = widgets.Label(value=rf\"0.000\")\n",
    "\n",
    "label = widgets.Label(\"\", layout=Layout(width=\"50px\"))\n",
    "\n",
    "label_kl_1 = widgets.Label(value=rf\"$KL[q(\\theta)||p(\\theta)]=$\")\n",
    "label_kl = widgets.Label(value=rf\"0.000\")\n",
    "\n",
    "label_kl2_1 = widgets.Label(value=rf\"$KL[q(\\theta)||p(\\theta|\\text{{Daten}})]=$\")\n",
    "label_kl2 = widgets.Label(value=rf\"0.000\")\n",
    "\n",
    "label_log_e_1 = widgets.Label(value=rf\"$\\mathbb{{E}}[\\text{{log}} \\ p(\\text{{Daten}}|\\theta)]=$\")\n",
    "label_log_e = widgets.Label(value=rf\"0.000\")\n",
    "\n",
    "info_label = widgets.Label()\n",
    "\n",
    "widgets.VBox([widgets.HBox([progr, btn, btn2, info_label]), widgets.HBox([out]), widgets.HBox([widgets.HBox([label_alpha_1, label_alpha]), label,  widgets.HBox([label_beta_1, label_beta]), label, widgets.HBox([label_kl_1, label_kl]), label, widgets.HBox([label_log_e_1, label_log_e]), label, widgets.HBox([label_kl2_1, label_kl2])])])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bnn",
   "language": "python",
   "name": "bnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
